{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from utils import init_mesh\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open(\"control_dicts/constant_properties.json\", \"r\") as openfile:\n",
    "    # Reading from json file\n",
    "    constant_properties = json.load(openfile)\n",
    "\n",
    "Db = constant_properties[\"Db\"]\n",
    "Dn = constant_properties[\"Dn\"]\n",
    "phi = constant_properties[\"phi\"]\n",
    "cb = constant_properties[\"cb\"]\n",
    "lambd_nb = constant_properties[\"lambd_nb\"]\n",
    "mi_n = constant_properties[\"mi_n\"]\n",
    "lambd_bn = constant_properties[\"lambd_bn\"]\n",
    "y_n = constant_properties[\"y_n\"]\n",
    "Cn_max = constant_properties[\"Cn_max\"]\n",
    "X_nb = constant_properties[\"X_nb\"]\n",
    "central_ini_cond = constant_properties[\"central_ini_cond\"]\n",
    "ini_cond_var = constant_properties[\"ini_cond_var\"]\n",
    "\n",
    "# Opening JSON file\n",
    "with open(\"control_dicts/mesh_properties.json\", \"r\") as openfile:\n",
    "    # Reading from json file\n",
    "    mesh_properties = json.load(openfile)\n",
    "\n",
    "h = mesh_properties[\"h\"]\n",
    "k = mesh_properties[\"k\"]\n",
    "x_dom = mesh_properties[\"x_dom\"]\n",
    "y_dom = mesh_properties[\"y_dom\"]\n",
    "t_dom = mesh_properties[\"t_dom\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100001, 2, 2, 21, 21)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_infection_site(struct_name):\n",
    "    center_str = (struct_name).split(\"__\")[-3].split(\"(\")[-1].split(\")\")[0].split(\",\")\n",
    "\n",
    "    center = (float(center_str[0]), float(center_str[1]))\n",
    "\n",
    "    radius = float((\"fvm_sim/Cp__\" + struct_name).split(\"__\")[-2].split(\"--\")[-1])\n",
    "\n",
    "    return center, radius\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    file_list = glob(path + \"/*\")\n",
    "\n",
    "    speed_up_list = []\n",
    "    Cb_list = []\n",
    "    Cn_list = []\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        variable = lambda a: a.split(\"/\")[-1].split(\"__\")[0]\n",
    "\n",
    "        if variable(file) == \"Cl\":\n",
    "            Cn_list.append(file)\n",
    "\n",
    "        elif variable(file) == \"Cp\":\n",
    "            Cb_list.append(file)\n",
    "\n",
    "        elif variable(file) == \"speed_up\":\n",
    "            speed_up_list.append(file)\n",
    "\n",
    "    return Cn_list, Cb_list, speed_up_list\n",
    "\n",
    "\n",
    "def change_dim_order(np_array):\n",
    "    # (2, 2, 100001, 21, 21)\n",
    "    sim_shape = np_array.shape\n",
    "\n",
    "    form_array = np.zeros(\n",
    "        (\n",
    "            sim_shape[2],\n",
    "            sim_shape[0],\n",
    "            sim_shape[1],\n",
    "            sim_shape[3],\n",
    "            sim_shape[4],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in range(sim_shape[2]):\n",
    "        form_array[i, :, :, :, :] = np_array[:, :, i, :, :]\n",
    "\n",
    "    return form_array\n",
    "\n",
    "\n",
    "def format_array(Cb_list, Cn_list):\n",
    "\n",
    "    for i, (Cb_file, Cn_file) in enumerate(zip(Cb_list, Cn_list)):\n",
    "        with open(Cb_file, \"rb\") as f:\n",
    "            new_Cb = pk.load(f)\n",
    "\n",
    "        with open(Cn_file, \"rb\") as f:\n",
    "            new_Cn = pk.load(f)\n",
    "\n",
    "        sim_shape = new_Cb.shape\n",
    "\n",
    "        if i == 0:\n",
    "            Cb = np.zeros(\n",
    "                (len(Cb_list), sim_shape[0], sim_shape[1], sim_shape[2], sim_shape[3])\n",
    "            )\n",
    "\n",
    "            Cn = np.zeros(\n",
    "                (len(Cn_list), sim_shape[0], sim_shape[1], sim_shape[2], sim_shape[3])\n",
    "            )\n",
    "\n",
    "            center_array = np.zeros((len(Cb_list), 2))\n",
    "\n",
    "            radius_array = np.zeros(len(Cb_list))\n",
    "\n",
    "        Cb[i, :, :, :, :] = new_Cb\n",
    "\n",
    "        Cn[i, :, :, :, :] = new_Cn\n",
    "\n",
    "        center, radius = get_infection_site(Cb_file)\n",
    "\n",
    "        center_array[i] = center\n",
    "\n",
    "        radius_array[i] = radius\n",
    "\n",
    "    formated_Cb = change_dim_order(Cb)\n",
    "\n",
    "    formated_Cn = change_dim_order(Cn)\n",
    "\n",
    "    return formated_Cb, formated_Cn, center_array, radius_array\n",
    "\n",
    "\n",
    "Cn_list, Cb_list, speed_up_list = read_files(\"fvm_sim\")\n",
    "\n",
    "Cb_fvm, Cn_fvm, center_array, radius_array = format_array(Cb_list,Cn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps in time = 100001\n",
      "Steps in space_x = 21\n",
      "Steps in space_y = 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_mesh_properties(\n",
    "    x_dom,\n",
    "    y_dom,\n",
    "    t_dom,\n",
    "    h,\n",
    "    k,\n",
    "    central_ini_cond,\n",
    "    ini_cond_var,\n",
    "    n_ini,\n",
    "    verbose=True,\n",
    "):\n",
    "\n",
    "    size_x = int(((x_dom[1] - x_dom[0]) / (h)) + 1)\n",
    "    size_y = int(((y_dom[1] - y_dom[0]) / (h)) + 1)\n",
    "    size_t = int(((t_dom[1] - t_dom[0]) / (k)) + 1)\n",
    "\n",
    "    initial_cond = np.linspace(\n",
    "        central_ini_cond * (1 - ini_cond_var),\n",
    "        central_ini_cond * (1 + ini_cond_var),\n",
    "        num=n_ini,\n",
    "        endpoint=True,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Steps in time = {:d}\\nSteps in space_x = {:d}\\nSteps in space_y = {:d}\\n\".format(\n",
    "                size_t,\n",
    "                size_x,\n",
    "                size_y,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return (size_x, size_y, size_t, initial_cond)\n",
    "\n",
    "\n",
    "size_x, size_y, size_t, initial_cond = get_mesh_properties(\n",
    "    x_dom,\n",
    "    y_dom,\n",
    "    t_dom,\n",
    "    h,\n",
    "    k,\n",
    "    central_ini_cond,\n",
    "    ini_cond_var,\n",
    "    len(Cb_fvm[0,0,:,0,0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n"
     ]
    }
   ],
   "source": [
    "t_np = np.linspace(t_dom[0], t_dom[-1], num=3, endpoint=True, dtype=np.float32)\n",
    "inf_site = np.linspace(0, len(center_array), num=len(radius_array), endpoint=True, dtype=np.float32)\n",
    "x_np = np.linspace(x_dom[0], x_dom[-1], num=size_x, endpoint=True, dtype=np.float32)\n",
    "y_np = np.linspace(y_dom[0], y_dom[-1], num=size_y, endpoint=True, dtype=np.float32)\n",
    "\n",
    "ii, ss, xx, yy = np.meshgrid(initial_cond, inf_site, x_np, y_np)\n",
    "print(len(ss.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_input_np = np.array([Cn_fdm.flatten(), Cb_fdm.flatten()]).T\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    t_tc = (\n",
    "        torch.tensor(tt, dtype=torch.float32, requires_grad=True)\n",
    "        .reshape(-1, 1)\n",
    "        .to(device)\n",
    "    )\n",
    "    x_tc = (\n",
    "        torch.tensor(xx, dtype=torch.float32, requires_grad=True)\n",
    "        .reshape(-1, 1)\n",
    "        .to(device)\n",
    "    )\n",
    "    y_tc = (\n",
    "        torch.tensor(yy, dtype=torch.float32, requires_grad=True)\n",
    "        .reshape(-1, 1)\n",
    "        .to(device)\n",
    "    )\n",
    "    data_input = torch.tensor(data_input_np, dtype=torch.float32).to(device)\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    t_tc = torch.tensor(tt, dtype=torch.float32, requires_grad=True).reshape(-1, 1)\n",
    "    x_tc = torch.tensor(xx, dtype=torch.float32, requires_grad=True).reshape(-1, 1)\n",
    "    y_tc = torch.tensor(yy, dtype=torch.float32, requires_grad=True).reshape(-1, 1)\n",
    "    data_input = torch.tensor(data_input_np, dtype=torch.float32)\n",
    "\n",
    "print(device)\n",
    "\n",
    "del xx\n",
    "del yy\n",
    "del tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archtecture handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dict = {\n",
    "    \"Elu\": nn.ELU,\n",
    "    \"LeakyReLU\": nn.LeakyReLU,\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "    \"Softplus\": nn.Softplus,\n",
    "    \"Tanh\": nn.Tanh,\n",
    "    \"Linear\": nn.Linear,\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"RReLU\": nn.RReLU,\n",
    "    \"SELU\": nn.SELU,\n",
    "    \"CELU\": nn.CELU,\n",
    "    \"GELU\": nn.GELU,\n",
    "    \"SiLU\": nn.SiLU,\n",
    "    \"GLU\": nn.GLU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_str = \"Tanh--32__Tanh--32__Tanh--32__GELU--32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(arch_str):\n",
    "    hidden_layers = arch_str.split(\"__\")\n",
    "\n",
    "    modules = []\n",
    "\n",
    "    for params in hidden_layers:\n",
    "        if len(params) != 0:\n",
    "            activation, out_neurons = params.split(\"--\")\n",
    "\n",
    "            if len(modules) == 0:\n",
    "                if activation == \"Linear\":\n",
    "                    modules.append(activation_dict[activation](3, int(out_neurons)))\n",
    "\n",
    "                else:\n",
    "                    modules.append(nn.Linear(3, int(out_neurons)))\n",
    "                    modules.append(activation_dict[activation]())\n",
    "\n",
    "            else:\n",
    "                if activation == \"Linear\":\n",
    "                    modules.append(\n",
    "                        activation_dict[activation](int(in_neurons), int(out_neurons))\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    modules.append(nn.Linear(int(in_neurons), int(out_neurons)))\n",
    "                    modules.append(activation_dict[activation]())\n",
    "\n",
    "            in_neurons = out_neurons\n",
    "\n",
    "    modules.append(nn.Linear(int(in_neurons), 2))\n",
    "\n",
    "    return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_model(arch_str).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_torch(dataset):\n",
    "    with torch.no_grad():\n",
    "        dt_min = torch.min(dataset, 0).values\n",
    "        dt_max = torch.max(dataset, 0).values\n",
    "        normalized = (dataset - dt_min) / (dt_max - dt_min)\n",
    "\n",
    "    return normalized.requires_grad_(True), dt_min, dt_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_input(data_input, steps):\n",
    "    with torch.no_grad():\n",
    "        dataset = data_input.reshape(steps, steps, 2)\n",
    "        normalized = torch.zeros_like(dataset)\n",
    "        for i in range(len(dataset)):\n",
    "            dt_min = torch.min(dataset[i], 0).values\n",
    "            dt_max = torch.max(dataset[i], 0).values\n",
    "            normalized[i] = (dataset[i] - dt_min) / (dt_max - dt_min)\n",
    "\n",
    "    return normalized.reshape((steps) * (steps), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(dataset, dt_min, dt_max):\n",
    "    return (dt_max - dt_min) * dataset + dt_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(x, y, z):\n",
    "    Data_num = np.arange(x.shape[0])\n",
    "    np.random.shuffle(Data_num)\n",
    "\n",
    "    return x[Data_num], y[Data_num], z[Data_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, z, test_size=0.5, shuffle=True):\n",
    "    with torch.no_grad():\n",
    "        if shuffle:\n",
    "            x, y, z = shuffle_data(x, y, z)\n",
    "        if test_size < 1:\n",
    "            train_ratio = len(x) - int(len(x) * test_size)\n",
    "            x_train, x_test = x[:train_ratio], x[train_ratio:]\n",
    "            y_train, y_test = y[:train_ratio], y[train_ratio:]\n",
    "            z_train, z_test = z[:train_ratio], z[train_ratio:]\n",
    "            return (\n",
    "                x_train.requires_grad_(True),\n",
    "                x_test.requires_grad_(True),\n",
    "                y_train.requires_grad_(True),\n",
    "                y_test.requires_grad_(True),\n",
    "                z_train.requires_grad_(True),\n",
    "                z_test.requires_grad_(True),\n",
    "            )\n",
    "        elif test_size in range(1, len(x)):\n",
    "            x_train, x_test = x[test_size:], x[:test_size]\n",
    "            y_train, y_test = y[test_size:], y[:test_size]\n",
    "            z_train, z_test = z[test_size:], z[:test_size]\n",
    "            return (\n",
    "                x_train.requires_grad_(True),\n",
    "                x_test.requires_grad_(True),\n",
    "                y_train.requires_grad_(True),\n",
    "                y_test.requires_grad_(True),\n",
    "                z_train.requires_grad_(True),\n",
    "                z_test.requires_grad_(True),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerenate_training_points(num_points, device):\n",
    "    x = torch.rand(num_points, 1).to(device)\n",
    "    y = torch.rand(num_points, 1).to(device)\n",
    "    t = torch.rand(num_points, 1).to(device) * 10\n",
    "\n",
    "    return t.requires_grad_(True), x.requires_grad_(True), y.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerenate_boundary_points(num_points, device):\n",
    "    x_boundary = torch.tensor([0.0, 1.0]).repeat(num_points // 2)\n",
    "    y_boundary = torch.rand(num_points)\n",
    "\n",
    "    if torch.rand(1) > 0.5:\n",
    "        x_boundary, y_boundary = y_boundary, x_boundary\n",
    "        n = torch.tensor([[0.0, -1.0], [0.0, 1.0]]).repeat(num_points // 2, 1)\n",
    "    else:\n",
    "        n = torch.tensor([[-1.0, 0.0], [1.0, 0.0]]).repeat(num_points // 2, 1)\n",
    "\n",
    "    return (\n",
    "        x_boundary.to(device).requires_grad_(True).view(-1, 1),\n",
    "        y_boundary.to(device).requires_grad_(True).view(-1, 1),\n",
    "        n.to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition(x, y):\n",
    "\n",
    "    Cl = torch.full_like(x, 0)\n",
    "\n",
    "    Cp = torch.full_like(x, 0)\n",
    "\n",
    "    for i, (xx, yy) in enumerate(zip(x, y)):\n",
    "        if ((xx >= 0.5) and (xx <= 0.6)) and ((yy >= 0.5) and (yy <= 0.6)):\n",
    "            Cp[i] = 0.2\n",
    "\n",
    "    return torch.cat([Cl, Cp], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_condition(t_b, x_b, y_b, n, model, Dn, X_nb, Db):\n",
    "\n",
    "    input_data = torch.cat([t_b, x_b, y_b], dim=1)\n",
    "\n",
    "    Cp, Cl = model(input_data).tensor_split(2, dim=1)\n",
    "\n",
    "    nx, ny = n.tensor_split(2, dim=1)\n",
    "\n",
    "    if nx[0].item() != 0:\n",
    "        dCp_dx = torch.autograd.grad(\n",
    "            Cp,\n",
    "            x_b,\n",
    "            grad_outputs=torch.ones_like(Cp),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )\n",
    "\n",
    "        dCl_dx = torch.autograd.grad(\n",
    "            Cl,\n",
    "            x_b,\n",
    "            grad_outputs=torch.ones_like(Cl),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )\n",
    "\n",
    "        Cl_boundary = torch.mul(\n",
    "            ((Dn * dCl_dx[0]) - X_nb * torch.mul(Cl, dCp_dx[0])), nx\n",
    "        )\n",
    "\n",
    "        Cp_boundary = torch.mul((Db * dCp_dx[0]), nx)\n",
    "\n",
    "        return torch.cat([Cl_boundary, Cp_boundary], dim=1)\n",
    "\n",
    "    else:\n",
    "        dCp_dy = torch.autograd.grad(\n",
    "            Cp,\n",
    "            y_b,\n",
    "            grad_outputs=torch.ones_like(Cp),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )\n",
    "\n",
    "        dCl_dy = torch.autograd.grad(\n",
    "            Cl,\n",
    "            y_b,\n",
    "            grad_outputs=torch.ones_like(Cl),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )\n",
    "\n",
    "        Cl_boundary = torch.mul(\n",
    "            ((Dn * dCl_dy[0]) - X_nb * torch.mul(Cl, dCp_dy[0])), ny\n",
    "        )\n",
    "\n",
    "        Cp_boundary = torch.mul((Db * dCp_dy[0]), ny)\n",
    "\n",
    "        return torch.cat([Cl_boundary, Cp_boundary], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde(model, t, x, y, cb, lambd_nb, Db, y_n, Cn_max, lambd_bn, mi_n, Dn, X_nb):\n",
    "\n",
    "    Cl, Cp = model(torch.cat([t, x, y], dim=1)).tensor_split(2, dim=1)\n",
    "\n",
    "    # Calculating Cp value\n",
    "\n",
    "    dCp_dx, dCp_dy = torch.autograd.grad(\n",
    "        Cp,\n",
    "        [x, y],\n",
    "        grad_outputs=torch.ones_like(Cp),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )\n",
    "\n",
    "    dCp_dx_2 = torch.autograd.grad(\n",
    "        dCp_dx,\n",
    "        x,\n",
    "        grad_outputs=torch.ones_like(dCp_dx),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    dCp_dy_2 = torch.autograd.grad(\n",
    "        dCp_dy,\n",
    "        y,\n",
    "        grad_outputs=torch.ones_like(dCp_dy),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    dCp_dt = torch.autograd.grad(\n",
    "        Cp,\n",
    "        t,\n",
    "        grad_outputs=torch.ones_like(Cp),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    qb = cb * Cp\n",
    "    rb = lambd_nb * torch.mul(Cl, Cp)\n",
    "\n",
    "    Cp_eq = Db * (dCp_dx_2 + dCp_dy_2) - rb + qb - dCp_dt\n",
    "\n",
    "    # Calculating Cl value\n",
    "\n",
    "    dCl_dx, dCl_dy = torch.autograd.grad(\n",
    "        Cl,\n",
    "        [x, y],\n",
    "        grad_outputs=torch.ones_like(Cl),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )\n",
    "\n",
    "    dCl_dx_2 = torch.autograd.grad(\n",
    "        dCl_dx,\n",
    "        x,\n",
    "        grad_outputs=torch.ones_like(dCl_dx),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    dCl_dy_2 = torch.autograd.grad(\n",
    "        dCl_dy,\n",
    "        y,\n",
    "        grad_outputs=torch.ones_like(dCl_dy),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    dCl_dt = torch.autograd.grad(\n",
    "        Cl,\n",
    "        t,\n",
    "        grad_outputs=torch.ones_like(Cl),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    qn = y_n * torch.mul(Cp, (Cn_max - Cl))\n",
    "    rn = lambd_bn * torch.mul(Cl, Cp) + mi_n * Cl\n",
    "\n",
    "    Cl_eq = (\n",
    "        Dn * (dCl_dx_2 + dCl_dy_2)\n",
    "        - X_nb\n",
    "        * (\n",
    "            (torch.mul(dCl_dx, dCp_dx) + torch.mul(Cl, dCp_dx_2))\n",
    "            + (torch.mul(dCl_dy, dCp_dy) + torch.mul(Cl, dCp_dy_2))\n",
    "        )\n",
    "        - rn\n",
    "        + qn\n",
    "    ) - dCl_dt\n",
    "\n",
    "    return torch.cat([Cl_eq, Cp_eq], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_epochs,\n",
    "        batch_size,\n",
    "        decay_rate,\n",
    "        model,\n",
    "        device,\n",
    "        data_input,\n",
    "        t,\n",
    "        x,\n",
    "        y,\n",
    "        n_points,\n",
    "        constant_properties,\n",
    "        norm_weights=None,\n",
    "        validation=None,\n",
    "        tolerance=None,\n",
    "        patience=10,\n",
    "    ):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.decay_rate = decay_rate\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.data_input = data_input\n",
    "        self.t = t\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_points = n_points\n",
    "        self.constant_properties = constant_properties\n",
    "        self.norm_weights = norm_weights\n",
    "        self.validation = validation\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "        pass\n",
    "\n",
    "    def loss_func(\n",
    "        self,\n",
    "    ):\n",
    "        # Computing intial loss\n",
    "        t_initial = torch.zeros_like(self.train_t[self.i : self.i + self.batch_size])\n",
    "\n",
    "        mesh_ini = torch.cat(\n",
    "            [\n",
    "                t_initial,\n",
    "                self.train_x[self.i : self.i + self.batch_size],\n",
    "                self.train_y[self.i : self.i + self.batch_size],\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        C_initial_pred = model(mesh_ini)\n",
    "\n",
    "        self.loss_initial = self.criterium(\n",
    "            self.C_initial[self.i : self.i + self.batch_size], C_initial_pred\n",
    "        )\n",
    "\n",
    "        # Computing pde loss\n",
    "\n",
    "        x_pde, y_pde, t_pde = gerenate_training_points(self.n_points, device)\n",
    "\n",
    "        predicted_pde = pde(\n",
    "            model,\n",
    "            t_pde,\n",
    "            x_pde,\n",
    "            y_pde,\n",
    "            self.constant_properties[\"cb\"],\n",
    "            self.constant_properties[\"lambd_nb\"],\n",
    "            self.constant_properties[\"Db\"],\n",
    "            self.constant_properties[\"y_n\"],\n",
    "            self.constant_properties[\"Cn_max\"],\n",
    "            self.constant_properties[\"lambd_bn\"],\n",
    "            self.constant_properties[\"mi_n\"],\n",
    "            self.constant_properties[\"Dn\"],\n",
    "            self.constant_properties[\"X_nb\"],\n",
    "        )\n",
    "\n",
    "        self.loss_pde = self.criterium(\n",
    "            predicted_pde,\n",
    "            torch.zeros_like(predicted_pde),\n",
    "        )\n",
    "\n",
    "        # Computing boundary loss\n",
    "\n",
    "        x_bnd, y_bnd, n_bnd = gerenate_boundary_points(self.n_points, device)\n",
    "\n",
    "        predicted_boundary = boundary_condition(\n",
    "            t_pde,\n",
    "            x_bnd,\n",
    "            y_bnd,\n",
    "            n_bnd,\n",
    "            model,\n",
    "            self.constant_properties[\"Dn\"],\n",
    "            self.constant_properties[\"X_nb\"],\n",
    "            self.constant_properties[\"Db\"],\n",
    "        )\n",
    "\n",
    "        self.loss_boundary = self.criterium(\n",
    "            predicted_boundary,\n",
    "            torch.zeros_like(predicted_boundary),\n",
    "        )\n",
    "\n",
    "        # Computing data loss\n",
    "\n",
    "        C_pred = model(self.train_data_input[self.i : self.i + self.batch_size])\n",
    "\n",
    "        self.loss_data = self.criterium(\n",
    "            self.train_data[self.i : self.i + self.batch_size], C_pred\n",
    "        )\n",
    "\n",
    "        self.loss = (\n",
    "            10 * self.loss_initial\n",
    "            + self.loss_pde\n",
    "            + self.loss_boundary\n",
    "            + self.loss_data * 10\n",
    "        )\n",
    "\n",
    "        self.loss.backward()\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "    ):\n",
    "        self.criterium = nn.MSELoss()\n",
    "\n",
    "        dt_min, dt_max = self.norm_weights if self.norm_weights else (0, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer=self.optimizer, gamma=self.decay_rate\n",
    "        )\n",
    "\n",
    "        if self.validation:\n",
    "            (\n",
    "                self.train_data,\n",
    "                self.test_data,\n",
    "                self.train_t,\n",
    "                self.test_t,\n",
    "                train_dom,\n",
    "                test_dom,\n",
    "            ) = train_test_split(\n",
    "                self.data_input,\n",
    "                self.t,\n",
    "                torch.cat([self.x, self.y], dim=1),\n",
    "                test_size=self.validation,\n",
    "            )\n",
    "            self.train_x, self.train_y = train_dom.split(1, dim=1)\n",
    "            self.test_x, self.test_y = test_dom.split(1, dim=1)\n",
    "            self.train_data_input = torch.cat(\n",
    "                [self.train_t, self.train_x, self.train_y], dim=1\n",
    "            )\n",
    "            self.test_data_input = torch.cat(\n",
    "                [self.test_t, self.test_x, self.test_y], dim=1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.train_data = data_input\n",
    "            self.test_data_input = None\n",
    "            self.train_t = self.t\n",
    "            self.test_t = None\n",
    "            self.train_x = self.x\n",
    "            self.test_x = None\n",
    "            self.train_y = self.y\n",
    "            self.test_y = None\n",
    "            self.test_data = None\n",
    "            self.train_data_input = torch.cat(\n",
    "                [self.t, self.train_x, self.train_y], dim=1\n",
    "            )\n",
    "\n",
    "        C_pde_loss_it = torch.zeros(self.n_epochs).to(device)\n",
    "        C_data_loss_it = torch.zeros(self.n_epochs).to(device)\n",
    "        C_boundary_loss_it = torch.zeros(self.n_epochs).to(device)\n",
    "        C_initial_loss_it = torch.zeros(self.n_epochs).to(device)\n",
    "        self.C_initial = initial_condition(self.train_x, self.train_y).to(device)\n",
    "        val_loss_it = torch.zeros(self.n_epochs).to(device)\n",
    "\n",
    "        patience_count = 0\n",
    "        val_loss = torch.tensor([1000])\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for self.i in range(0, len(self.train_t), self.batch_size):\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                self.optimizer.step(self.loss_func)\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            # Computing validation loss\n",
    "\n",
    "            if self.validation:\n",
    "                with torch.no_grad():\n",
    "                    val_old = val_loss\n",
    "                    val_loss = self.criterium(\n",
    "                        self.test_data, model(self.test_data_input)\n",
    "                    )\n",
    "\n",
    "            C_pde_loss_it[epoch] = self.loss_pde.item()\n",
    "            C_boundary_loss_it[epoch] = self.loss_boundary.item()\n",
    "            C_initial_loss_it[epoch] = self.loss_initial.item()\n",
    "            C_data_loss_it[epoch] = self.loss_data.item()\n",
    "            val_loss_it[epoch] = val_loss.item() if self.validation else 0\n",
    "\n",
    "            if ((epoch + 1) % 100) == 0 or (epoch == 0):\n",
    "                print(\n",
    "                    f\"Finished epoch {epoch+1}, latest loss {self.loss}, validation loss {val_loss.item()}\"\n",
    "                    if self.validation\n",
    "                    else f\"Finished epoch {epoch+1}, latest loss {self.loss}\"\n",
    "                )\n",
    "\n",
    "            if self.tolerance:\n",
    "\n",
    "                if (\n",
    "                    abs(val_old.item() - val_loss.item()) / val_old.item()\n",
    "                    < self.tolerance\n",
    "                ):\n",
    "                    patience_count += 1\n",
    "\n",
    "                else:\n",
    "                    patience_count = 0\n",
    "\n",
    "                if patience_count >= self.patience:\n",
    "\n",
    "                    C_pde_loss_it = C_pde_loss_it[:epoch]\n",
    "                    C_boundary_loss_it = C_boundary_loss_it[:epoch]\n",
    "                    C_initial_loss_it = C_initial_loss_it[:epoch]\n",
    "                    C_data_loss_it = C_data_loss_it[:epoch]\n",
    "                    val_loss_it = val_loss_it[:epoch]\n",
    "\n",
    "                    print(\"Early break!\")\n",
    "\n",
    "                    break\n",
    "\n",
    "        return (\n",
    "            model,\n",
    "            C_pde_loss_it,\n",
    "            C_boundary_loss_it,\n",
    "            C_initial_loss_it,\n",
    "            C_data_loss_it,\n",
    "            val_loss_it,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = 0.9985\n",
    "n_epochs = 600\n",
    "batch_size = 71440\n",
    "\n",
    "trainer = train(\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    decay_rate=decay_rate,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    data_input=data_input,\n",
    "    x=x_tc,\n",
    "    y=y_tc,\n",
    "    t=t_tc,\n",
    "    n_points=batch_size,\n",
    "    constant_properties=constant_properties,\n",
    "    validation=0.1,\n",
    "    tolerance=0.001,\n",
    "    patience=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    model,\n",
    "    C_pde_loss_it,\n",
    "    C_boundary_loss_it,\n",
    "    C_initial_loss_it,\n",
    "    C_data_loss_it,\n",
    "    val_loss_it,\n",
    ") = trainer.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=[18, 9])\n",
    "\n",
    "fig.suptitle(\"Curva de aprendizagem\", fontsize=16)\n",
    "\n",
    "# Plotango 3D\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.set_xlabel(\"iterações\")\n",
    "ax.set_ylabel(\"perda\")\n",
    "ax.plot(\n",
    "    range(len(C_pde_loss_it.cpu().numpy())),\n",
    "    C_pde_loss_it.cpu().numpy(),\n",
    "    label=\"PDE loss\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(len(C_boundary_loss_it.cpu().numpy())),\n",
    "    C_boundary_loss_it.cpu().numpy(),\n",
    "    label=\"Boundary loss\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(len(val_loss_it.cpu().numpy())),\n",
    "    val_loss_it.cpu().numpy(),\n",
    "    label=\"Validation loss\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(len(C_data_loss_it.cpu().numpy())),\n",
    "    C_data_loss_it.cpu().numpy(),\n",
    "    label=\"Data loss\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(len(C_initial_loss_it.cpu().numpy())),\n",
    "    C_initial_loss_it.cpu().numpy(),\n",
    "    label=\"Initial loss\",\n",
    ")\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=[18, 9])\n",
    "\n",
    "fig.suptitle(\"Curva de aprendizagem\", fontsize=16)\n",
    "\n",
    "# Plotango 3D\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.set_xlabel(\"iterações\")\n",
    "ax.set_ylabel(\"perda\")\n",
    "\n",
    "ax.plot(\n",
    "    range(len(C_initial_loss_it.cpu().numpy())),\n",
    "    C_initial_loss_it.cpu().numpy()\n",
    "    + C_pde_loss_it.cpu().numpy()\n",
    "    + C_boundary_loss_it.cpu().numpy()\n",
    "    + C_data_loss_it.cpu().numpy(),\n",
    "    label=\"PINN loss\",\n",
    ")\n",
    "ax.plot(\n",
    "    range(len(val_loss_it.cpu().numpy())),\n",
    "    val_loss_it.cpu().numpy(),\n",
    "    label=\"Validation loss\",\n",
    ")\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print model's state_dict\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms state_dict:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(param_tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mstate_dict()[param_tensor]\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "torch.save(model.state_dict(), cwd + \"/nn_parameters/\" + arch_str + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "model = generate_model(arch_str).to(device)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(cwd + \"/nn_parameters/\" + arch_str + \".pt\", weights_only=True)\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvm_model_parallel import cu_solve_pde\n",
    "import math\n",
    "from numba import cuda\n",
    "\n",
    "speed_up = []\n",
    "\n",
    "for i in range(33):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Definindo número de threads e blocos para cuda\n",
    "\n",
    "    threadsperblock = (size_x, size_y)\n",
    "    blockspergrid_x = math.ceil(size_x / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(size_y / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    # Inicializando as matrizes para concentrações de neutrófilos (Cn) e bactérias (Cb)\n",
    "\n",
    "    Cb_buf_0 = cuda.to_device(np.zeros((size_x, size_y)))\n",
    "    Cn_buf_0 = cuda.to_device(np.zeros((size_x, size_y)))\n",
    "\n",
    "    # This extra array is used for synchronization purposes\n",
    "\n",
    "    Cb_buf_1 = cuda.device_array_like(Cb_buf_0)\n",
    "    Cn_buf_1 = cuda.device_array_like(Cn_buf_0)\n",
    "\n",
    "    # Matrizes para armazenar as concentrações em cada passo de tempo\n",
    "    Cb_final_device = cuda.to_device(np.zeros((size_t, size_x, size_y)))\n",
    "    Cn_final_device = cuda.to_device(np.zeros((size_t, size_x, size_y)))\n",
    "\n",
    "    cu_solve_pde[threadsperblock, blockspergrid](\n",
    "        Cb_buf_0,\n",
    "        Cn_buf_0,\n",
    "        Cb_buf_1,\n",
    "        Cn_buf_1,\n",
    "        Cb_final_device,\n",
    "        Cn_final_device,\n",
    "        size_t,\n",
    "        size_x,\n",
    "        size_y,\n",
    "        h,\n",
    "        k,\n",
    "        Db,\n",
    "        Dn,\n",
    "        phi,\n",
    "        cb,\n",
    "        lambd_nb,\n",
    "        mi_n,\n",
    "        lambd_bn,\n",
    "        y_n,\n",
    "        Cn_max,\n",
    "        X_nb,\n",
    "    )\n",
    "\n",
    "    Cb_host = np.empty(shape=Cb_final_device.shape, dtype=Cb_final_device.dtype)\n",
    "    Cb_final_device.copy_to_host(Cb_host)\n",
    "\n",
    "    Cn_host = np.empty(shape=Cn_final_device.shape, dtype=Cn_final_device.dtype)\n",
    "    Cn_final_device.copy_to_host(Cn_host)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    cuda_time = end - start\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    mesh = torch.cat([t_tc, x_tc, y_tc], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Cl_pinn_device, Cp_pinn_device = model(mesh).split(1, dim=1)\n",
    "\n",
    "    Cl_pinn = Cl_pinn_device.cpu().detach().numpy()\n",
    "    Cp_pinn = Cp_pinn_device.cpu().detach().numpy()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    pinn_time = end - start\n",
    "\n",
    "    speed_up.append(cuda_time / pinn_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_speed_up = np.mean(speed_up)\n",
    "std_speed_up = np.std(speed_up)\n",
    "\n",
    "rmse = np.mean(\n",
    "    [\n",
    "        ((Cl_p[0] - Cl_f) ** 2 + (Cp_p[0] - Cp_f) ** 2) ** 0.5\n",
    "        for Cl_p, Cp_p, Cl_f, Cp_f in zip(\n",
    "            Cl_pinn, Cp_pinn, Cn_fdm.flatten(), Cb_fdm.flatten()\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "max_ae = np.max(\n",
    "    [\n",
    "        [((Cl_p[0] - Cl_f) ** 2) ** 0.5, ((Cp_p[0] - Cp_f) ** 2) ** 0.5]\n",
    "        for Cl_p, Cp_p, Cl_f, Cp_f in zip(\n",
    "            Cl_pinn, Cp_pinn, Cn_fdm.flatten(), Cb_fdm.flatten()\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"rmse\": rmse,\n",
    "    \"max_ae\": max_ae,\n",
    "    \"mean_speed_up\": mean_speed_up,\n",
    "    \"std_speed_up\": std_speed_up,\n",
    "    \"Cl_pinn\": Cl_pinn,\n",
    "    \"Cp_pinn\": Cp_pinn,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Erro absoluto médio\", rmse)\n",
    "print(\"Erro absoluto máximo\", max_ae)\n",
    "print(\"Speed Up: {} +/-{}\".format(mean_speed_up, std_speed_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cl_pinn_np = Cl_pinn.reshape(size_t, size_x, size_y)\n",
    "Cp_pinn_np = Cp_pinn.reshape(size_t, size_x, size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(x_dom[0], x_dom[1], num=size_x, endpoint=True)\n",
    "y = np.linspace(y_dom[0], y_dom[1], num=size_y, endpoint=True)\n",
    "t = np.linspace(t_dom[0], t_dom[1], num=size_t, endpoint=True)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "time_plot = np.linspace(0, len(t) - 1, num=5, endpoint=True, dtype=int)\n",
    "\n",
    "fig = plt.figure(figsize=[6 * len(time_plot), 14])\n",
    "\n",
    "fig.suptitle(\"Resposta imunológica a patógenos\", fontsize=16)\n",
    "\n",
    "for i, time_inst in enumerate(time_plot):\n",
    "    # Plotango 3D\n",
    "    ax = fig.add_subplot(4, len(time_plot), i + 1)\n",
    "\n",
    "    vmin = np.min(Cp_pinn_np)\n",
    "    vmax = np.max(Cp_pinn_np)\n",
    "\n",
    "    contour = ax.contourf(\n",
    "        X,\n",
    "        Y,\n",
    "        Cp_pinn_np[time_inst],\n",
    "        cmap=\"jet\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Con. de bactérias em t={}\".format(t[time_inst]))\n",
    "    ax.set_xlabel(\"tempo\")\n",
    "    ax.set_ylabel(\"cond. inicial\")\n",
    "    colobar = fig.colorbar(contour, ticks=np.linspace(vmin, vmax, num=5))\n",
    "\n",
    "for i, time_inst in enumerate(time_plot):\n",
    "    # Plotango 3D\n",
    "    ax = fig.add_subplot(4, len(time_plot), i + len(time_plot) + 1)\n",
    "\n",
    "    vmin = np.min(Cl_pinn_np)\n",
    "    vmax = np.max(Cl_pinn_np)\n",
    "\n",
    "    contour = ax.contourf(\n",
    "        X,\n",
    "        Y,\n",
    "        Cl_pinn_np[time_inst],\n",
    "        cmap=\"jet\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Con. de bactérias em t={}\".format(t[time_inst]))\n",
    "    ax.set_xlabel(\"tempo\")\n",
    "    ax.set_ylabel(\"cond. inicial\")\n",
    "    colobar = fig.colorbar(contour, ticks=np.linspace(vmin, vmax, num=5))\n",
    "\n",
    "for i, it in enumerate(time_plot):\n",
    "    # Plotango 3D\n",
    "    ax = fig.add_subplot(4, len(time_plot), i + len(time_plot) * 2 + 1)\n",
    "\n",
    "    vmin = np.min(Cb_fdm)\n",
    "    vmax = np.max(Cb_fdm)\n",
    "\n",
    "    contour = ax.contourf(\n",
    "        X,\n",
    "        Y,\n",
    "        Cb_fdm[it],\n",
    "        cmap=\"jet\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Con. de bactérias em t={}\".format(t[it]))\n",
    "    ax.set_xlabel(\"tempo\")\n",
    "    ax.set_ylabel(\"cond. inicial\")\n",
    "    colobar = fig.colorbar(contour, ticks=np.linspace(vmin, vmax, num=5))\n",
    "\n",
    "for i, it in enumerate(time_plot):\n",
    "    # Plotango 3D\n",
    "    ax = fig.add_subplot(4, len(time_plot), i + len(time_plot) * 3 + 1)\n",
    "\n",
    "    vmin = np.min(Cn_fdm)\n",
    "    vmax = np.max(Cn_fdm)\n",
    "\n",
    "    contour = ax.contourf(\n",
    "        X,\n",
    "        Y,\n",
    "        Cn_fdm[it],\n",
    "        cmap=\"jet\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Con. de bactérias em t={}\".format(t[it]))\n",
    "    ax.set_xlabel(\"tempo\")\n",
    "    ax.set_ylabel(\"cond. inicial\")\n",
    "    colobar = fig.colorbar(contour, ticks=np.linspace(vmin, vmax, num=5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
